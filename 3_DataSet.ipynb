{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. DataSet",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNtyRX1QRX33b5jRYoUJ4lj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iskra3138/ImageSr/blob/master/3_DataSet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bu8xxiSBOVk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "60b11e5e-e304-4036-acfc-c8523012a22d"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekIoYkbefOWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQDv63u_3Ku-",
        "colab_type": "text"
      },
      "source": [
        "### TFRecord 활용하여 DataSet 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNegY3FQBQXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CLASSES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGIOYhjZBQUx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "# TFRecord Parsing을 위한 함수 정의  \n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "IMG_WIDTH = 224\n",
        "IMG_HEIGHT = 224\n",
        "IMAGE_SIZE =  [IMG_HEIGHT, IMG_WIDTH]\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "## 본 실험에서는 16개의 tfrecord파일을 train/validation용으로 나눠서 사용합니다.\n",
        "## train전용, validation전용 tfrecord 파일들이 있으면 특정해서 list 로 넘기시면 됩니다.\n",
        "BUCKET = \"gs://iskra3138_share\"\n",
        "gcs_pattern = os.path.join(BUCKET, '*.tfrec')\n",
        "validation_split = 0.19\n",
        "filenames = tf.io.gfile.glob(gcs_pattern)\n",
        "split = len(filenames) - int(len(filenames) * validation_split)\n",
        "train_fns = filenames[:split]\n",
        "validation_fns = filenames[split:]\n",
        "\n",
        "## TFRecord Parsing 함수 (TFRecord 생성함수를 참고해서 만들어줍니다.)\n",
        "def parse_tfrecord(example):\n",
        "    features = {\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string),  # tf.string = bytestring (not text string)\n",
        "        \"file_name\": tf.io.FixedLenFeature([], tf.string),  # one bytestring\n",
        "        \"label_name\": tf.io.FixedLenFeature([], tf.string),  # one bytestring\n",
        "        \"label\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means scalar, one integer\n",
        "    }\n",
        "    # decode the TFRecord\n",
        "    example = tf.io.parse_single_example(example, features)\n",
        "    \n",
        "    \n",
        "    label = example['label']\n",
        "    label = tf.one_hot(indices=label, depth=5)   \n",
        "    image = tf.image.decode_jpeg(example['image'], channels=3)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32) ## make [0,255] to [0,1) resize 앞에 위치할 때만 [0,1), 즉 input이 float32가 아니어야 작동\n",
        "    image = tf.image.resize(image, IMAGE_SIZE) ## method가 tf.image.ResizeMethod.NEAREST_NEIGHBOR 가 아니면 출력은 무조건 float32\n",
        "    \n",
        "    file_name  = example['file_name']\n",
        "    label_name  = example['label_name']\n",
        "    \n",
        "    return image, label, file_name, label_name\n",
        "\n",
        "def load_dataset(filenames):\n",
        "  # Read from TFRecords. For optimal performance, we interleave reads from multiple files.\n",
        "  records = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
        "  return records.map(parse_tfrecord, num_parallel_calls=AUTO)\n",
        "\n",
        "def get_training_dataset():\n",
        "  dataset = load_dataset(train_fns)\n",
        "\n",
        "  # Create some additional training images by randomly flipping and\n",
        "  # increasing/decreasing the saturation of images in the training set. \n",
        "  def data_augment(image, label, file_name, label_name):\n",
        "    modified = tf.image.random_flip_left_right(image)\n",
        "    modified = tf.image.random_flip_up_down(modified)\n",
        "    return modified, label, file_name, label_name\n",
        "  augmented = dataset.map(data_augment, num_parallel_calls=AUTO)\n",
        "\n",
        "  # Prefetch the next batch while training (autotune prefetch buffer size).\n",
        "  return augmented.repeat().shuffle(2048).batch(batch_size).prefetch(AUTO) \n",
        "\n",
        "training_dataset = get_training_dataset()\n",
        "validation_dataset = load_dataset(validation_fns).batch(batch_size).prefetch(AUTO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J9m6MIOcR0h",
        "colab_type": "text"
      },
      "source": [
        "위 코드를 실행시키면, training_dataset, validation_dataset 모두 (image, label, file_name, label_name)구조로 element들이 구성됨\n",
        "\n",
        "training_dataset은 \n",
        "- augmented = dataset.map(data_augment, num_parallel_calls=AUTO)에서 랜덤으로 좌우플립/상하플립이 적용되고, \n",
        "- augmented.repeat().shuffle(2048).batch(batch_size).prefetch(AUTO) 에서 무한 반복으로, 섞고, batch_size만큼 생성됨\n",
        "\n",
        "validation_dataset은 \n",
        "- batch_size만큼 뽑히는 데, 전체 element들이 다 뽑히면 끝남"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_qIfsQ73RsL",
        "colab_type": "text"
      },
      "source": [
        "### TFrecord 파일 속에 있는 데이터들 개수 세기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDd47OjnBQSI",
        "colab_type": "code",
        "outputId": "d89fecc1-8d2f-47ff-9123-53c9c8aa0294",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## tfrecord 파일 내 데이터 개수 세기\n",
        "#### tfrecord 이름이 아니라 직접 데이터 개수를 확인하고 싶다면 아래 코드 실행\n",
        "#sum([1 for _ in tf.data.TFRecordDataset({TFRecord 파일 이름})])\n",
        "sum([1 for _ in tf.data.TFRecordDataset(filenames[0])])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "230"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJcmn9vfXWnT",
        "colab_type": "code",
        "outputId": "4c865e4a-5ead-41d9-ce60-7adad83206b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## tfrecord 파일 내 데이터 개수 세기\n",
        "#### tfrecord 이름이 아니라 직접 데이터 개수를 확인하고 싶다면 아래 코드 실행\n",
        "#### 여러 개의 tfrecord를 list로 넘겨도 됨\n",
        "\n",
        "#sum([1 for _ in tf.data.TFRecordDataset([TFRecord 파일 리스트])])\n",
        "sum([1 for _ in tf.data.TFRecordDataset(filenames[0:3])])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "690"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_5BHOMJdW1c",
        "colab_type": "code",
        "outputId": "679a9a9e-d475-4839-cb91-f5242d9a15e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "## train_fns list로 traind_data 개수 세기\n",
        "print (train_fns)\n",
        "print (sum([1 for _ in tf.data.TFRecordDataset(train_fns)]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['gs://iskra3138_share/00-230.tfrec', 'gs://iskra3138_share/01-230.tfrec', 'gs://iskra3138_share/02-230.tfrec', 'gs://iskra3138_share/03-230.tfrec', 'gs://iskra3138_share/04-230.tfrec', 'gs://iskra3138_share/05-230.tfrec', 'gs://iskra3138_share/06-230.tfrec', 'gs://iskra3138_share/07-230.tfrec', 'gs://iskra3138_share/08-230.tfrec', 'gs://iskra3138_share/09-230.tfrec', 'gs://iskra3138_share/10-230.tfrec', 'gs://iskra3138_share/11-230.tfrec', 'gs://iskra3138_share/12-230.tfrec']\n",
            "2990\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7D8TS0odWxA",
        "colab_type": "code",
        "outputId": "b2f757f9-729c-4fed-ab00-1bea4bfaf257",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "## validation_fns list로 traind_data 개수 세기\n",
        "print (validation_fns)\n",
        "print (sum([1 for _ in tf.data.TFRecordDataset(validation_fns)]))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['gs://iskra3138_share/13-230.tfrec', 'gs://iskra3138_share/14-230.tfrec', 'gs://iskra3138_share/15-220.tfrec']\n",
            "680\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA2I8cJcdwBy",
        "colab_type": "text"
      },
      "source": [
        "train data는 2990장, validation data는 680장임"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L44MZqs23Wh_",
        "colab_type": "text"
      },
      "source": [
        "### 1batch 데이터 n 개 추출하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72xqsmsqeHXP",
        "colab_type": "code",
        "outputId": "84836ff2-9edb-40f5-8450-14f69bd6f140",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## dataset 랜덤으로 1batch 데이터 뽑기 1\n",
        "## as_numpy_iterator()를 써서 next로 뽑기\n",
        "iter = validation_dataset.as_numpy_iterator()\n",
        "img, label, file_name, label_name = next(iter)\n",
        "label1 = label\n",
        "print (label1.shape)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkJIG2PLx8SQ",
        "colab_type": "code",
        "outputId": "ada1de6d-38f7-4f89-bc35-292958ea7d59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# as_numpy_iterator()의 출력을 numpy array\n",
        "type(label1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PBREmm-es9G",
        "colab_type": "code",
        "outputId": "3d61a14c-52bd-4243-a1b1-45fceaf7b315",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## dataset 랜덤으로 1batch 데이터 뽑기 2\n",
        "### dataset 자체를 쓰되, break\n",
        "### break를 쓰지 않으면 as_numpy_iterator()를 for문으로 돌릴때처럼 전체 데이터셋에 대해 한 바퀴 돔\n",
        "for img, label, file_name, label_name in validation_dataset :\n",
        "  label2 = label\n",
        "  break\n",
        "print (label2.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBdD0DjYes5S",
        "colab_type": "code",
        "outputId": "7bb14816-d695-42fc-9a1a-b0aaf1494645",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# take()의 출력을 EagerTensor\n",
        "type(label2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.framework.ops.EagerTensor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m9gIeNnDHS7",
        "colab_type": "code",
        "outputId": "49703816-f42f-42c7-c63c-6146adb238da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## dataset 랜덤으로 1batch 데이터 뽑기 3\n",
        "### take() 사용\n",
        "### take(n)을 쓰면 사전에 정의된 배치사이즈 크기의 데이터를 n번 샘플링 해옴\n",
        "for img, label, file_name, label_name in validation_dataset.take(1):\n",
        "  label3 = label\n",
        "  print (label3.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wbsvb1F2x2h0",
        "colab_type": "code",
        "outputId": "fc13756b-013f-4346-b145-183a28619693",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# take()의 출력을 EagerTensor\n",
        "type(label3)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.framework.ops.EagerTensor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9bMytHLyimT",
        "colab_type": "code",
        "outputId": "1dc4ee14-ad0c-42a2-cd97-4f0fad104b8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# 순서가 명시된 Dataset 정의\n",
        "# (0,10), (1, 11), ..., (9, 19) 형태의 DataSet을 만듦\n",
        "a = np.arange(10)\n",
        "b = np.arange(10, 20)\n",
        "print (a)\n",
        "print (b)\n",
        "range_dataset = tf.data.Dataset.from_tensor_slices((a, b))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 3 4 5 6 7 8 9]\n",
            "[10 11 12 13 14 15 16 17 18 19]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2WkGOHEzXva",
        "colab_type": "code",
        "outputId": "cc3e7826-b8c5-422b-d6b7-0bc381a911bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "## dataset 랜덤으로 1batch 데이터 뽑기 1\n",
        "## as_numpy_iterator()를 써서 next로 뽑기\n",
        "iter = range_dataset.as_numpy_iterator()\n",
        "for i in range(10):\n",
        "  data = next(iter)\n",
        "  print (data)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, 10)\n",
            "(1, 11)\n",
            "(2, 12)\n",
            "(3, 13)\n",
            "(4, 14)\n",
            "(5, 15)\n",
            "(6, 16)\n",
            "(7, 17)\n",
            "(8, 18)\n",
            "(9, 19)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEtvrc6dzJnx",
        "colab_type": "code",
        "outputId": "26571e2f-9284-47a6-f171-b6d0e32cc601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## dataset 랜덤으로 1batch 데이터 뽑기 2\n",
        "### take() 사용\n",
        "### 출력값은 EagerTensor의 Tuple이므로 출력 결과 중 numpy= 다음의 값을 보면 됨\n",
        "for data in range_dataset.take(1):\n",
        "  print (data)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: shape=(), dtype=int64, numpy=0>, <tf.Tensor: shape=(), dtype=int64, numpy=10>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OZoEK39zXsQ",
        "colab_type": "code",
        "outputId": "4245c7a6-6e31-4106-fc11-2d83a2d208ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "## dataset 랜덤으로 1batch 데이터 뽑기 2\n",
        "### take() 사용시 numpy 값을 뽑고 싶다면 요소별로 변수로 받은 다음 numpy() 하면 됨\n",
        "\n",
        "for data1, data2 in range_dataset.take(10):\n",
        "  print (data1.numpy(), data2.numpy())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 10\n",
            "1 11\n",
            "2 12\n",
            "3 13\n",
            "4 14\n",
            "5 15\n",
            "6 16\n",
            "7 17\n",
            "8 18\n",
            "9 19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH3kesH01Axp",
        "colab_type": "code",
        "outputId": "ccb445e1-ca0e-475e-fa1e-dc0a01e71fa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        }
      },
      "source": [
        "## as_numpy_iterator()를 정의해서 사용할 때 1epoch이 넘어가서도 호출하면 error 발생\n",
        "iter = range_dataset.as_numpy_iterator()\n",
        "for i in range(11): ## data의 개수 10개를 넘어가면 error 발생\n",
        "  data = next(iter)\n",
        "  print (data)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, 10)\n",
            "(1, 11)\n",
            "(2, 12)\n",
            "(3, 13)\n",
            "(4, 14)\n",
            "(5, 15)\n",
            "(6, 16)\n",
            "(7, 17)\n",
            "(8, 18)\n",
            "(9, 19)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "StopIteration",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   1896\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1897\u001b[0;31m     \u001b[0;32myield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1898\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    658\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2478\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2479\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2480\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6605\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6606\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6607\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mOutOfRangeError\u001b[0m: End of sequence [Op:IteratorGetNextSync]",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    664\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_compatible_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/context.py\u001b[0m in \u001b[0;36mexecution_mode\u001b[0;34m(mode)\u001b[0m\n\u001b[1;32m   1899\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecutor_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1900\u001b[0;31m     \u001b[0mexecutor_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/eager/executor.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;34m\"\"\"Waits for ops dispatched in this executor to finish.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFE_ExecutorWaitForAllPendingNodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfRangeError\u001b[0m: End of sequence",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-326defdb78ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_numpy_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m## data의 개수 10개를 넘어가면 error 발생\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3641\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3642\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3638\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3639\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3641\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mStopIteration\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ird9aX9A1Awk",
        "colab_type": "code",
        "outputId": "28cb8d01-35ef-46ce-eebd-8a81fc9efd42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "## take()를 사용하면 10개를 넘어가도 작동함, 10개에서 작동이 정지\n",
        "\n",
        "for data1, data2 in range_dataset.take(20):\n",
        "  print (data1.numpy(), data2.numpy())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 10\n",
            "1 11\n",
            "2 12\n",
            "3 13\n",
            "4 14\n",
            "5 15\n",
            "6 16\n",
            "7 17\n",
            "8 18\n",
            "9 19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEmoB2--3sHf",
        "colab_type": "text"
      },
      "source": [
        "###  DataSet 내에 있는 모든 데이터를 한번씩만 사용하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukXBDo2ugBId",
        "colab_type": "code",
        "outputId": "f1dff080-1b52-4b20-aca7-875b9bf4ee9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# TFRecord Data Set으로 재실험\n",
        "## as_numpy_iterator()를 쓰면 딱 1epoch만큼 샘플링할 수 있음\n",
        "## 64 batch씩 10번 남은 40batch 1번, 총 11번 반복함\n",
        "num_samples = 0\n",
        "for img, label, file_name, label_name in validation_dataset.as_numpy_iterator():\n",
        "  num_samples += label.shape[0]\n",
        "print (num_samples)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "680\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u8MU6uiigiOP"
      },
      "source": [
        "전체 데이터 개수와 똑같아졌음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOexvF0RgVK6",
        "colab_type": "code",
        "outputId": "6a501941-1c44-477f-c05f-319b70357869",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# TFRecord Data Set으로 재실험\n",
        "## dataset  자체를 써도 딱 1epoch만큼 샘플링할 수 있음 (차이는 아웃풋이 Tensor임)\n",
        "## 64 batch씩 10번 남은 40batch 1번, 총 11번 반복함\n",
        "num_samples = 0\n",
        "for img, label, file_name, label_name in validation_dataset:\n",
        "  num_samples += label.shape[0]\n",
        "print (num_samples)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "680\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3cZnl8CgzBm",
        "colab_type": "text"
      },
      "source": [
        "전체 데이터 개수와 똑같아졌음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ze39cenXgWLk",
        "colab_type": "code",
        "outputId": "004c4e91-7017-4e4c-d84c-f8a8531f1a57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "## take(n)를 쓰면 계속해서 증가하다가 1epoch 샘플링에서 끝남\n",
        "for n in range(15) :\n",
        "  num_samples = 0\n",
        "  for img, label, file_name, label_name in validation_dataset.take(n):\n",
        "    num_samples += label.shape[0]\n",
        "  print (n, num_samples)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0\n",
            "1 64\n",
            "2 128\n",
            "3 192\n",
            "4 256\n",
            "5 320\n",
            "6 384\n",
            "7 448\n",
            "8 512\n",
            "9 576\n",
            "10 640\n",
            "11 680\n",
            "12 680\n",
            "13 680\n",
            "14 680\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIoqFiCYhkMS",
        "colab_type": "text"
      },
      "source": [
        "계속해서 64개씩 늘어나다 11번째는 남은 40만큼 늘어나고 그 이후는 변화가 없음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqH-djRH30mf",
        "colab_type": "text"
      },
      "source": [
        "### 여러 Class가 섞여 있는 DataSet에서 Class 별 DataSet 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U58DiBQMDHNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class별 데이터세트 만들기\n",
        "## 원하는 클래스끼리만 모아 놓고 싶을 때는 fliter를 적용하면 됨\n",
        "### label이 1인 데이터세트를 만드는 예제\n",
        "\n",
        "def select_label1 (img, label, file_name, label_name):\n",
        "  return tf.math.equal(tf.argmax(label), 1) ## label이 one-hot 형태임\n",
        "\n",
        "label1dataset = validation_dataset.unbatch().filter(select_label1) ## 한 장씩 평가해야 해서 unbatch 해줌"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ftcVWPTBH3C",
        "colab_type": "code",
        "outputId": "8ebfca70-b733-433e-e391-dc8dfc1c8a5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "for img, label, file_name, label_name in label1dataset.take(10):\n",
        "  print (label.numpy())"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0. 1. 0. 0. 0.]\n",
            "[0. 1. 0. 0. 0.]\n",
            "[0. 1. 0. 0. 0.]\n",
            "[0. 1. 0. 0. 0.]\n",
            "[0. 1. 0. 0. 0.]\n",
            "[0. 1. 0. 0. 0.]\n",
            "[0. 1. 0. 0. 0.]\n",
            "[0. 1. 0. 0. 0.]\n",
            "[0. 1. 0. 0. 0.]\n",
            "[0. 1. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BTOXBdbkDPhh"
      },
      "source": [
        "### DataSet에서 Class 별 추출비율 정해서 DataSet 만들기\n",
        "[공식가이드](https://www.tensorflow.org/guide/data#resampling)\n",
        "- class1, class2 인 데이터들만 5:5로 재조합한 DataSet 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i76yx6G5Nj4O",
        "colab_type": "text"
      },
      "source": [
        "실제 클래스별 비율 보기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FDK2KAiFs43",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 클래스별 개수 세기\n",
        "## 데이터셋에 맞게 조정해서 사용\n",
        "def count(counts, batch):\n",
        "  img, label, file_name, label_name = batch\n",
        "  \n",
        "  class_0 = tf.math.equal(tf.argmax(label), 0) # label이 one-hot 형태라 이렇게 해줌, class_idx, 즉 sparse 형태면 공식 가이드 참조\n",
        "  class_0 = tf.cast(class_0, tf.int32)\n",
        "\n",
        "  class_1 = tf.math.equal(tf.argmax(label), 1)\n",
        "  class_1 = tf.cast(class_1, tf.int32)\n",
        "  \n",
        "  class_2 = tf.math.equal(tf.argmax(label), 2)\n",
        "  class_2 = tf.cast(class_2, tf.int32)\n",
        "\n",
        "  class_3 = tf.math.equal(tf.argmax(label), 3)\n",
        "  class_3 = tf.cast(class_3, tf.int32)\n",
        "  \n",
        "  class_4 = tf.math.equal(tf.argmax(label), 4)\n",
        "  class_4 = tf.cast(class_4, tf.int32)\n",
        "  \n",
        "  counts['class_0'] += tf.reduce_sum(class_0)\n",
        "  counts['class_1'] += tf.reduce_sum(class_1)\n",
        "  counts['class_2'] += tf.reduce_sum(class_2)\n",
        "  counts['class_3'] += tf.reduce_sum(class_3)\n",
        "  counts['class_4'] += tf.reduce_sum(class_4)\n",
        "\n",
        "  return counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIKmT8ouFpdh",
        "colab_type": "code",
        "outputId": "e59b1674-3157-4218-edf6-9fa080c4f308",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## take(n) 이므로 n개의 데이터에 대해서 비율을 계산함, \n",
        "### 앞서 봤듯이 n 값은 실제 데이터 개수보다 커도 되므로, 전체를 보고 싶으면 적당히 큰 값을 넣으면 됨\n",
        "counts = validation_dataset.take(10000).reduce(\n",
        "    initial_state={'class_0': 0, 'class_1': 0, 'class_2': 0, 'class_3': 0, 'class_4': 0},\n",
        "    reduce_func = count)\n",
        "\n",
        "counts = np.array([counts['class_0'].numpy(),\n",
        "                   counts['class_1'].numpy(),\n",
        "                   counts['class_2'].numpy(),\n",
        "                   counts['class_3'].numpy(),\n",
        "                   counts['class_4'].numpy()]).astype(np.float32)\n",
        "\n",
        "fractions = counts/counts.sum()\n",
        "print(fractions)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.30555555 0.19444445 0.22222222 0.1388889  0.1388889 ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIGUIXBJEC9e",
        "colab_type": "text"
      },
      "source": [
        "방법 1: Datasets sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6rS7YC-bDPhl",
        "colab": {}
      },
      "source": [
        "# 앞서 살펴본 class별 데이터세트들을 만들고 tf.data.experimental.sample_from_datasets 적용\n",
        "\n",
        "def select_label1 (img, label, file_name, label_name):\n",
        "  return tf.math.equal(tf.argmax(label), 1) ## label이 one-hot 형태임\n",
        "\n",
        "def select_label2 (img, label, file_name, label_name):\n",
        "  return tf.math.equal(tf.argmax(label), 2) ## label이 one-hot 형태임\n",
        "\n",
        "# 클래스별 데이터셋 생성\n",
        "label1dataset = validation_dataset.unbatch().filter(select_label1) ## 한 장씩 평가해야 해서 unbatch 해줌\n",
        "label2dataset = validation_dataset.unbatch().filter(select_label2) ## 한 장씩 평가해야 해서 unbatch 해줌\n",
        "\n",
        "balanced_ds = tf.data.experimental.sample_from_datasets([label1dataset, label2dataset], [0.5, 0.5]).batch(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoRFhd6fNAiK",
        "colab_type": "text"
      },
      "source": [
        "Now the dataset produces examples of each class with 50/50 probability:\n",
        "\n",
        "- 말 그대로 주어진 확률로 샘플링하는 것이므로 결과물은 정확히 5:5가 아닐 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ac27d4d2-b8c7-4a7d-828f-458239c2b6e7",
        "id": "82ff1xGaDPho",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "for img, label, file_name, label_name in balanced_ds.take(10):\n",
        "  print (np.argmax(label.numpy(), axis=1))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 1 1 2 2 1 1 1 2 2]\n",
            "[2 2 1 2 1 1 2 1 2 1]\n",
            "[1 2 1 1 2 1 2 1 1 1]\n",
            "[1 2 1 2 2 2 2 2 2 2]\n",
            "[1 1 1 1 2 2 2 1 1 2]\n",
            "[1 2 2 2 2 1 2 2 2 1]\n",
            "[1 1 1 2 2 2 1 2 1 1]\n",
            "[1 1 1 1 1 1 1 2 1 1]\n",
            "[1 1 1 1 2 2 1 2 1 2]\n",
            "[1 2 2 1 2 1 2 1 1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RmOD1hkIE4Tk"
      },
      "source": [
        "방법 2: Rejection resampling\n",
        "- 방법 1의 단점은 클래스별 데이터셋을 따로 만들어야 하므로 만들어야 하는 데이터셋 개수 만큼 데이터 로딩이 발생\n",
        "- 두 번째 방법은 한번만 로딩하면서 불필요한 거는 버리는 형태\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjZpYShING8e",
        "colab_type": "text"
      },
      "source": [
        "The resampler also needs a target distribution, and optionally an initial distribution estimate:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cExEwneCE4To",
        "colab": {}
      },
      "source": [
        "# label을 보고 샘플링 하므로 label만 반납하는 함수 정의\n",
        "def class_func(img, label, file_name, label_name):\n",
        "  return tf.argmax(label) # label이 one-hot이라 argmax로 반환, class_idx 형태면 그냥 label만 반납\n",
        "\n",
        "resampler = tf.data.experimental.rejection_resample(\n",
        "    class_func, target_dist=[0.0, 0.5, 0.5, 0.0, 0.0], initial_dist=fractions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsKvyDz-NYsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# label을 보고 샘플링 하므로 label만 반납하는 함수 정의\n",
        "def class_func(img, label, file_name, label_name):\n",
        "  return tf.argmax(label) # label이 one-hot이라 argmax로 반환, class_idx 형태면 그냥 label만 반납\n",
        "\n",
        "resampler = tf.data.experimental.rejection_resample(\n",
        "    class_func, target_dist=[0.0, 0.5, 0.5, 0.0, 0.0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAvtGlYbNEVN",
        "colab_type": "text"
      },
      "source": [
        "The resampler deals with individual examples, so you must unbatch the dataset before applying the resampler:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJSnynQ3GoyI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resample_ds = validation_dataset.unbatch().apply(resampler).batch(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NF-TBcTMrle",
        "colab_type": "text"
      },
      "source": [
        "The resampler returns creates (class, example) pairs from the output of the class_func. In this case, the example was already a (feature, label) pair, so use map to drop the extra copy of the labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DkjzVFvL1Ih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "balanced_ds = resample_ds.map(lambda extra_label, features_and_label: features_and_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP7C_K6xMyDe",
        "colab_type": "text"
      },
      "source": [
        "Now the dataset produces examples of each class with 50/50 probability:\n",
        "\n",
        "- 말 그대로 주어진 확률로 샘플링하는 것이므로 결과물은 정확히 5:5가 아닐 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "5f01accf-bc83-4d20-e57a-2e3bed0b9ce9",
        "id": "edtknQacE4Tr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "for img, label, file_name, label_name in balanced_ds.take(10):\n",
        "  print (np.argmax(label.numpy(), axis=1))"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2 1 1 1 2 2 1 1 1 1]\n",
            "[2 1 1 2 2 1 1 1 1 1]\n",
            "[1 2 2 2 2 1 1 2 1 2]\n",
            "[2 1 2 1 2 2 2 1 1 1]\n",
            "[1 1 2 1 2 1 1 2 1 2]\n",
            "[2 1 2 1 1 2 1 1 2 2]\n",
            "[2 2 1 2 2 2 2 1 1 1]\n",
            "[1 1 1 2 2 1 1 1 1 1]\n",
            "[1 2 2 1 2 2 1 1 1 2]\n",
            "[2 1 1 2 2 2 1 2 2 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgLe9caXK8ml",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}