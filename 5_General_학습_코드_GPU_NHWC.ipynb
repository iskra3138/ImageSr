{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "5. General 학습 코드_GPU_NHWC",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iskra3138/ImageSr/blob/master/5_General_%ED%95%99%EC%8A%B5_%EC%BD%94%EB%93%9C_GPU_NHWC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_xF6eZB8gnE",
        "colab_type": "text"
      },
      "source": [
        "[원문 출처](https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/07_Keras_Flowers_TPU_xception_fine_tuned_best.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AaBhh_vrE7j",
        "colab_type": "code",
        "outputId": "cfc5c45c-093b-43c1-a1ac-459e0c6d0eee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        }
      },
      "source": [
        "# GPU확인하고 고르기\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Feb 19 22:43:51 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.48.02    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "89B27-TGiDNB"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9u3d4Z7uQsmp",
        "outputId": "0a072c4a-09ff-4e00-90f5-560938c777b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import re, sys, time\n",
        "\n",
        "if 'google.colab' in sys.modules: # Colab-only Tensorflow version selector\n",
        "  %tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "AUTO = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Tensorflow version 2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mPo10cahZXXQ"
      },
      "source": [
        "## TPU or GPU detection\n",
        "TPUClusterResolver() automatically detects a connected TPU on all Gooogle's\n",
        "platforms: Colaboratory, AI Platform (ML Engine), Kubernetes, Kaggle, ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDkOJT_SaMBC",
        "colab_type": "text"
      },
      "source": [
        "experimental이 붙어 있는 API들은 실험코드라 나중에 Version이 바뀌면 사라지거나, 이름이 바뀔 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FpvUOuC3j27n",
        "outputId": "9a3c4d96-b290-4f8d-8124-d14ff981ef1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Detect hardware, return appropriate distribution strategy\n",
        "try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "except ValueError:\n",
        "    tpu = None\n",
        "    gpus = tf.config.experimental.list_logical_devices(\"GPU\") # 'experimental'이라 나중에 바뀔 수 있음 (TF2.1.0 작동 확인)\n",
        "    \n",
        "if tpu:\n",
        "    tf.config.experimental_connect_to_cluster(tpu) # 'experimental'이라 나중에 바뀔 수 있음 (TF2.1.0 작동 확인)   \n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu) # 'experimental'이라 나중에 바뀔 수 있음 (TF2.1.0 작동 확인)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu) # 'experimental'이라 나중에 바뀔 수 있음 (TF2.1.0 작동 확인)\n",
        "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "\n",
        "elif len(gpus) > 1:\n",
        "  strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n",
        "  print('Running on multiple GPUs ', [gpu.name for gpu in gpus])\n",
        "\n",
        "elif len(gpus) == 1:\n",
        "  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "  print('Running on single GPU ', gpus[0].name)\n",
        "\n",
        "else:\n",
        "  strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "  print('Running on CPU')\n",
        "\n",
        "print(\"REPLICAS(Number of accelerators): \", strategy.num_replicas_in_sync)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running on single GPU  /device:GPU:0\n",
            "REPLICAS(Number of accelerators):  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "MPkvHdAYNt9J",
        "colab": {}
      },
      "source": [
        "#@title display utilities [RUN ME]\n",
        "\n",
        "def dataset_to_numpy_util(dataset, N):\n",
        "  dataset = dataset.unbatch().batch(N)\n",
        "  for images, labels in dataset:\n",
        "    numpy_images = images.numpy()\n",
        "    numpy_labels = labels.numpy()\n",
        "    break;  \n",
        "  return numpy_images, numpy_labels\n",
        "\n",
        "def title_from_label_and_target(label, correct_label):\n",
        "  label = np.argmax(label, axis=-1)  # one-hot to class number\n",
        "  correct_label = np.argmax(correct_label, axis=-1) # one-hot to class number\n",
        "  correct = (label == correct_label)\n",
        "  return \"{} [{}{}{}]\".format(CLASSES[label], str(correct), ', shoud be ' if not correct else '',\n",
        "                              CLASSES[correct_label] if not correct else ''), correct\n",
        "\n",
        "def display_one_flower(image, title, subplot, red=False):\n",
        "    plt.subplot(subplot)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(image)\n",
        "    plt.title(title, fontsize=16, color='red' if red else 'black')\n",
        "    return subplot+1\n",
        "  \n",
        "def display_9_images_from_dataset(dataset):\n",
        "  subplot=331\n",
        "  plt.figure(figsize=(13,13))\n",
        "  images, labels = dataset_to_numpy_util(dataset, 9)\n",
        "  for i, image in enumerate(images):\n",
        "    title = CLASSES[np.argmax(labels[i], axis=-1)]\n",
        "    subplot = display_one_flower(image, title, subplot)\n",
        "    if i >= 8:\n",
        "      break;\n",
        "              \n",
        "  plt.tight_layout()\n",
        "  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "  plt.show()\n",
        "  \n",
        "def display_9_images_with_predictions(images, predictions, labels):\n",
        "  subplot=331\n",
        "  plt.figure(figsize=(13,13))\n",
        "  for i, image in enumerate(images):\n",
        "    title, correct = title_from_label_and_target(predictions[i], labels[i])\n",
        "    subplot = display_one_flower(image, title, subplot, not correct)\n",
        "    if i >= 8:\n",
        "      break;\n",
        "              \n",
        "  plt.tight_layout()\n",
        "  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "  plt.show()\n",
        "  \n",
        "def display_training_curves(training, validation, title, subplot):\n",
        "  if subplot%10==1: # set up the subplots on the first call\n",
        "    plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n",
        "    plt.tight_layout()\n",
        "  ax = plt.subplot(subplot)\n",
        "  ax.set_facecolor('#F8F8F8')\n",
        "  ax.plot(training)\n",
        "  ax.plot(validation)\n",
        "  ax.set_title('model '+ title)\n",
        "  ax.set_ylabel(title)\n",
        "  #ax.set_ylim(0.28,1.05)\n",
        "  ax.set_xlabel('epoch')\n",
        "  ax.legend(['train', 'valid.'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "w9S3uKC_iXY5"
      },
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e6VtaC2oaYK",
        "colab_type": "text"
      },
      "source": [
        "HyperParameter Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M3G-2aUBQJ-H",
        "outputId": "9445be82-5a4c-4645-b4d5-dd87055f16ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "# 장비별 하이퍼파라미터 셋팅: 본 예제에 사전 세팅된 값이므로 적당히 바꿔서 사용\n",
        "\n",
        "EPOCHS = 12\n",
        "\n",
        "if strategy.num_replicas_in_sync == 8 or len(gpus) > 1: # TPU or Multi GPUs\n",
        "    BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "    VALIDATION_BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "    start_lr = 0.00001\n",
        "    min_lr = 0.00001\n",
        "    max_lr = 0.00005 * strategy.num_replicas_in_sync\n",
        "    rampup_epochs = 5\n",
        "    sustain_epochs = 0\n",
        "    exp_decay = .8\n",
        "    \n",
        "elif strategy.num_replicas_in_sync == 1: # GPU\n",
        "    BATCH_SIZE = 48 #16\n",
        "    VALIDATION_BATCH_SIZE = 48 #16\n",
        "    start_lr = 0.00001\n",
        "    min_lr = 0.00001\n",
        "    max_lr = 0.0002\n",
        "    rampup_epochs = 5\n",
        "    sustain_epochs = 0\n",
        "    exp_decay = .8\n",
        "    \n",
        "else: # TPU pod\n",
        "    BATCH_SIZE = 8 * strategy.num_replicas_in_sync\n",
        "    VALIDATION_BATCH_SIZE = 8 * strategy.num_replicas_in_sync\n",
        "    start_lr = 0.00001\n",
        "    min_lr = 0.00001\n",
        "    max_lr = 0.00002 * strategy.num_replicas_in_sync\n",
        "    rampup_epochs = 7\n",
        "    sustain_epochs = 0\n",
        "    exp_decay = .8\n",
        "\n",
        "    \n",
        "def lrfn(epoch):\n",
        "    def lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay):\n",
        "        if epoch < rampup_epochs:\n",
        "            lr = (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n",
        "        elif epoch < rampup_epochs + sustain_epochs:\n",
        "            lr = max_lr\n",
        "        else:\n",
        "            lr = (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n",
        "        return lr\n",
        "    return lr(epoch, start_lr, min_lr, max_lr, rampup_epochs, sustain_epochs, exp_decay)\n",
        "    \n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=True)\n",
        "\n",
        "rng = [i for i in range(EPOCHS)]\n",
        "y = [lrfn(x) for x in rng]\n",
        "plt.plot(rng, [lrfn(x) for x in rng])\n",
        "print(y[0], y[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1e-05 5.9807360000000024e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAD4CAYAAAApWAtMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOroXQg9oVWb",
        "colab_type": "text"
      },
      "source": [
        "Four images sizes are available for this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jDta8E8kxGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data Check\n",
        "\n",
        "IMAGE_SIZE = [331, 331]\n",
        "\n",
        "FLOWERS_DATASETS = { # available image sizes\n",
        "    192: 'gs://flowers-public/tfrecords-jpeg-192x192-2/*.tfrec',\n",
        "    224: 'gs://flowers-public/tfrecords-jpeg-224x224/*.tfrec',\n",
        "    331: 'gs://flowers-public/tfrecords-jpeg-331x331/*.tfrec',\n",
        "    512: 'gs://flowers-public/tfrecords-jpeg-512x512/*.tfrec'\n",
        "}\n",
        "CLASSES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips'] # do not change, maps to the labels in the data (folder names)\n",
        "assert IMAGE_SIZE[0] == IMAGE_SIZE[1], \"only square images are supported\"\n",
        "assert IMAGE_SIZE[0] in FLOWERS_DATASETS, \"this image size is not supported\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypwf-eXaOmAT",
        "colab_type": "code",
        "outputId": "b188b6aa-db41-42a2-e16a-74777585bf95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "## GPU로 할 때는 GCP를 굳이 안써도 되므로 VM으로 Copy해서 사용하는 코드로 전환함\n",
        "## 필요한 TFRecord 파일을 VM으로 Copy\n",
        "!gsutil cp 'gs://flowers-public/tfrecords-jpeg-331x331/*.tfrec' ./"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://flowers-public/tfrecords-jpeg-331x331/flowers00-230.tfrec...\n",
            "Copying gs://flowers-public/tfrecords-jpeg-331x331/flowers01-230.tfrec...\n",
            "Copying gs://flowers-public/tfrecords-jpeg-331x331/flowers02-230.tfrec...\n",
            "Copying gs://flowers-public/tfrecords-jpeg-331x331/flowers03-230.tfrec...\n",
            "- [4 files][ 52.9 MiB/ 52.9 MiB]                                                \n",
            "==> NOTE: You are performing a sequence of gsutil operations that may\n",
            "run significantly faster if you instead use gsutil -m cp ... Please\n",
            "see the -m section under \"gsutil help options\" for further information\n",
            "about when gsutil -m can be advantageous.\n",
            "\n",
            "Copying gs://flowers-public/tfrecords-jpeg-331x331/flowers04-230.tfrec...\n",
            "Copying gs://flowers-public/tfrecords-jpeg-331x331/flowers05-230.tfrec...\n",
            "Copying gs://flowers-public/tfrecords-jpeg-331x331/flowers06-230.tfrec...\n",
            "Copying gs://flowers-public/tfrecords-jpeg-331x331/flowers07-230.tfrec...\n",
            "Copying gs://flowers-public/tfrecords-jpeg-331x331/flowers08-230.tfrec...\n",
            "Copying gs://flowers-public/tfrecords-jpeg-331x331/flowers09-230.tfrec...\n",
            "Copying gs://flowers-public/tfrecords-jpeg-331x331/flowers10-230.tfrec...\n",
            "Copying gs://flowers-public/tfrecords-jpeg-331x331/flowers11-230.tfrec...\n",
            "Copying gs://flowers-public/tfrecords-jpeg-331x331/flowers12-230.tfrec...\n",
            "Copying gs://flowers-public/tfrecords-jpeg-331x331/flowers13-230.tfrec...\n",
            "Copying gs://flowers-public/tfrecords-jpeg-331x331/flowers14-230.tfrec...\n",
            "Copying gs://flowers-public/tfrecords-jpeg-331x331/flowers15-220.tfrec...\n",
            "| [16 files][209.2 MiB/209.2 MiB]                                               \n",
            "Operation completed over 16 objects/209.2 MiB.                                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kvPXiovhi3ZZ"
      },
      "source": [
        "## Read images and labels from TFRecords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LtAVr-4CP1rp",
        "outputId": "f6030eb8-cd93-451b-cf0c-c513b2281a5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# tfrecord이름에 포함된 파일 개수 counting하는 함수\n",
        "def count_data_items(filenames):\n",
        "    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
        "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
        "    return np.sum(n)\n",
        "\n",
        "# Split train data and validation data\n",
        "gcs_pattern = FLOWERS_DATASETS[IMAGE_SIZE[0]]\n",
        "validation_split = 0.19\n",
        "\n",
        "# GCP 버킷에 있는 파일을 쓸 때\n",
        "#filenames = tf.io.gfile.glob(gcs_pattern)\n",
        "# VM Local에 있는 파일을 쓸 때\n",
        "filenames = tf.io.gfile.glob('./*.tfrec')\n",
        "\n",
        "split = len(filenames) - int(len(filenames) * validation_split)\n",
        "TRAIN_FILENAMES = filenames[:split]\n",
        "VALID_FILENAMES = filenames[split:]\n",
        "\n",
        "# Keras로 학습할 때 필요한 train_steps 계산\n",
        "TRAIN_STEPS = count_data_items(TRAIN_FILENAMES) // BATCH_SIZE\n",
        "print(\"TRAINING IMAGES: \", count_data_items(TRAIN_FILENAMES), \", STEPS PER EPOCH: \", TRAIN_STEPS)\n",
        "print(\"VALIDATION IMAGES: \", count_data_items(VALID_FILENAMES))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TRAINING IMAGES:  2990 , STEPS PER EPOCH:  62\n",
            "VALIDATION IMAGES:  680\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "22rVDTx8wCqE"
      },
      "source": [
        "## training and validation datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP2xFhy2qZVk",
        "colab_type": "text"
      },
      "source": [
        "TFRecord 파일 기반 DataSet 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8mlRFd7qX1L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## TFRecord 파일 Parsing \n",
        "#### TFRecord 생성 코드 참고해서 정의함\n",
        "#### TFRecord에 포함된 데이터들 중 필요한 것들만 Return\n",
        "def read_tfrecord(example):\n",
        "    features = {\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
        "        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means scalar\n",
        "        \"one_hot_class\": tf.io.VarLenFeature(tf.float32),\n",
        "    }\n",
        "    example = tf.io.parse_single_example(example, features)\n",
        "    image = tf.image.decode_jpeg(example['image'], channels=3)  \n",
        "    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
        "    #image2 = tf.transpose(image, [2, 0, 1]) # HWC -> CHW\n",
        "    \n",
        "    class_label = tf.cast(example['class'], tf.int32)\n",
        "    one_hot_class = tf.sparse.to_dense(example['one_hot_class'])\n",
        "    one_hot_class = tf.reshape(one_hot_class, [5])\n",
        "    return image, one_hot_class\n",
        "    \n",
        "\n",
        "  \n",
        "def force_image_sizes(dataset, image_size):\n",
        "    # explicit size will be needed for TPU\n",
        "    reshape_images = lambda image, label: (tf.reshape(image, [*image_size, 3]), label)\n",
        "    dataset = dataset.map(reshape_images, num_parallel_calls=AUTO)\n",
        "    return dataset\n",
        "\n",
        "def load_dataset(filenames):\n",
        "    # read from TFRecords. For optimal performance, use \"interleave(tf.data.TFRecordDataset, ...)\"\n",
        "    # to read from multiple TFRecord files at once and set the option experimental_deterministic = False\n",
        "    # to allow order-altering optimizations.\n",
        "\n",
        "    opt = tf.data.Options()\n",
        "    opt.experimental_deterministic = False\n",
        "\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
        "    dataset = dataset.with_options(opt)\n",
        "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
        "    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n",
        "    dataset = force_image_sizes(dataset, IMAGE_SIZE)\n",
        "    return dataset\n",
        "\n",
        "def data_augment(image, one_hot_class):\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    image = tf.image.random_saturation(image, 0, 2)\n",
        "    return image, one_hot_class\n",
        "\n",
        "def make_NCHW(image, one_hot_class):\n",
        "    image = tf.transpose(image, [2, 0, 1]) # HWC -> CHW\n",
        "    return image, one_hot_class  \n",
        "# For experts: fine adjustments of tf.data.Dataset distribution behavior:\n",
        "\n",
        "# Replicating a datset with state (even random number generator state) does not replicate the\n",
        "# state and changes the behavior of the dataset. If the state is just the RNG state, it usually\n",
        "# does not matter but this behavior can be adjusted with tf.data.experimental.ExternalStatePolicy:\n",
        "#  WARN = 0   (this is the default in Tensorflow outside of Keras)\n",
        "#  IGNORE = 1 (this is the default in Keras)\n",
        "#  FAIL = 2\n",
        "\n",
        "# On TPU pods, the dataset API attempts to shard the dataset across individual TPUs at the file\n",
        "# level so that TPUs only load the data they will actually train on. This requires more data files \n",
        "# than TPUs in the pod. (ex: TPU v3-32 pod = 4 TPUs => dataset must have at least 4 files)\n",
        "# An error will occur if there are not enough data files. File-level sharding can be disabled:\n",
        "#  opt = tf.data.Options()\n",
        "#  opt.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
        "#  dataset = dataset.with_options(opt)\n",
        "   \n",
        "\n",
        "def get_training_dataset():\n",
        "    dataset = load_dataset(TRAIN_FILENAMES)\n",
        "    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n",
        "    dataset = dataset.map(make_NCHW, num_parallel_calls=AUTO)\n",
        "    dataset = dataset.repeat()\n",
        "    dataset = dataset.shuffle(2048)\n",
        "    dataset = dataset.batch(BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    return dataset\n",
        "\n",
        "def get_validation_dataset():\n",
        "    dataset = load_dataset(VALID_FILENAMES)\n",
        "    dataset = dataset.map(make_NCHW, num_parallel_calls=AUTO)\n",
        "    dataset = dataset.batch(VALIDATION_BATCH_SIZE)\n",
        "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
        "    \n",
        "    # needed for TPU 32-core pod: the test dataset has only 3 files but there are 4 TPUs. FILE sharding policy must be disabled.\n",
        "    opt = tf.data.Options()\n",
        "    opt.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
        "    dataset = dataset.with_options(opt)\n",
        "    \n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7wxKyCklR4Gh",
        "colab": {}
      },
      "source": [
        "training_dataset = get_training_dataset()\n",
        "validation_dataset = get_validation_dataset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7k92o92qm4e",
        "colab_type": "text"
      },
      "source": [
        "생성된 DataSet 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xb-b4PRz-V6O",
        "outputId": "6b5eb026-50c2-4a43-f6be-e7a8f2005205",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "display_9_images_from_dataset(validation_dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-111-9b60fb61b335>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay_9_images_from_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-0e942debfe52>\u001b[0m in \u001b[0;36mdisplay_9_images_from_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCLASSES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0msubplot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisplay_one_flower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-0e942debfe52>\u001b[0m in \u001b[0;36mdisplay_one_flower\u001b[0;34m(image, title, subplot, red)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mred\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'black'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msubplot\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2682\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2683\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[0;32m-> 2684\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2685\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2686\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1597\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    367\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5677\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5679\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5680\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5681\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    688\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[1;32m    689\u001b[0m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0;32m--> 690\u001b[0;31m                             .format(self._A.shape))\n\u001b[0m\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid shape (3, 331, 331) for image data"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAN4AAADeCAYAAABSZ763AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAACgElEQVR4nO3TMQEAIAzAMMC/5+GCHiQK+nTPzALe\nOnUA/Mh4EDAeBIwHAeNBwHgQMB4EjAcB40HAeBAwHgSMBwHjQcB4EDAeBIwHAeNBwHgQMB4EjAcB\n40HAeBAwHgSMBwHjQcB4EDAeBIwHAeNBwHgQMB4EjAcB40HAeBAwHgSMBwHjQcB4EDAeBIwHAeNB\nwHgQMB4EjAcB40HAeBAwHgSMBwHjQcB4EDAeBIwHAeNBwHgQMB4EjAcB40HAeBAwHgSMBwHjQcB4\nEDAeBIwHAeNBwHgQMB4EjAcB40HAeBAwHgSMBwHjQcB4EDAeBIwHAeNBwHgQMB4EjAcB40HAeBAw\nHgSMBwHjQcB4EDAeBIwHAeNBwHgQMB4EjAcB40HAeBAwHgSMBwHjQcB4EDAeBIwHAeNBwHgQMB4E\njAcB40HAeBAwHgSMBwHjQcB4EDAeBIwHAeNBwHgQMB4EjAcB40HAeBAwHgSMBwHjQcB4EDAeBIwH\nAeNBwHgQMB4EjAcB40HAeBAwHgSMBwHjQcB4EDAeBIwHAeNBwHgQMB4EjAcB40HAeBAwHgSMBwHj\nQcB4EDAeBIwHAeNBwHgQMB4EjAcB40HAeBAwHgSMBwHjQcB4EDAeBIwHAeNBwHgQMB4EjAcB40HA\neBAwHgSMBwHjQcB4EDAeBIwHAeNBwHgQMB4EjAcB40HAeBAwHgSMBwHjQcB4EDAeBIwHAeNBwHgQ\nMB4EjAcB40HAeBAwHgSMBwHjQcB4EDAeBIwHAeNBwHgQMB4EjAcB40HAeBAwHgSMBwHjQcB4EDAe\nBIwHAeNBwHgQMB4EjAcB40HAeBAwHgSMBwHjQcB4EDAeBIwHAeNB4AL23wS53Ah8rgAAAABJRU5E\nrkJggg==\n",
            "text/plain": [
              "<Figure size 936x936 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ALtRUlxhw8Vt"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJl3vNtJOB-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "    #pretrained_model = tf.keras.applications.MobileNetV2(input_shape=[*IMAGE_SIZE, 3], include_top=False)\n",
        "    pretrained_model = tf.keras.applications.Xception(input_shape=[3, *IMAGE_SIZE], include_top=False)\n",
        "    #pretrained_model = tf.keras.applications.VGG16(weights='imagenet', include_top=False ,input_shape=[*IMAGE_SIZE, 3])\n",
        "    #pretrained_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=[*IMAGE_SIZE, 3])\n",
        "    #pretrained_model = tf.keras.applications.MobileNet(weights='imagenet', include_top=False, input_shape=[*IMAGE_SIZE, 3])\n",
        "    pretrained_model.trainable = True\n",
        "    \n",
        "    '''\n",
        "    model = tf.keras.Sequential([\n",
        "        pretrained_model,\n",
        "        tf.keras.layers.GlobalAveragePooling2D(),\n",
        "        #tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(5, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    '''\n",
        "    # GradCAM같이 pretraind model의 중간 레이어가 필요할 경우에는 Functional하게 정의해줍니다.\n",
        "    x = pretrained_model.output\n",
        "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "    predictions = tf.keras.layers.Dense(5, activation='softmax', name='prediction') (x)\n",
        "    model = tf.keras.Model(inputs=pretrained_model.input, outputs=predictions)\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss = 'categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XLJNVGwHUDy1",
        "outputId": "1ee11da6-9c77-4bb2-c7b7-b832f670db7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "with strategy.scope(): # creating the model in the TPUStrategy scope places the model on the TPU\n",
        "    model = create_model()\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 3, 331, 331) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, 32, 165, 165) 864         input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_bn (BatchNormaliza (None, 32, 165, 165) 128         block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1_act (Activation)   (None, 32, 165, 165) 0           block1_conv1_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, 64, 163, 163) 18432       block1_conv1_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_bn (BatchNormaliza (None, 64, 163, 163) 256         block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2_act (Activation)   (None, 64, 163, 163) 0           block1_conv2_bn[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1 (SeparableConv2 (None, 128, 163, 163 8768        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv1_bn (BatchNormal (None, 128, 163, 163 512         block2_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_act (Activation (None, 128, 163, 163 0           block2_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2 (SeparableConv2 (None, 128, 163, 163 17536       block2_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block2_sepconv2_bn (BatchNormal (None, 128, 163, 163 512         block2_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 128, 82, 82)  8192        block1_conv2_act[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, 128, 82, 82)  0           block2_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 128, 82, 82)  512         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_48 (Add)                    (None, 128, 82, 82)  0           block2_pool[0][0]                \n",
            "                                                                 batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_act (Activation (None, 128, 82, 82)  0           add_48[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1 (SeparableConv2 (None, 256, 82, 82)  33920       block3_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv1_bn (BatchNormal (None, 256, 82, 82)  1024        block3_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_act (Activation (None, 256, 82, 82)  0           block3_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2 (SeparableConv2 (None, 256, 82, 82)  67840       block3_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block3_sepconv2_bn (BatchNormal (None, 256, 82, 82)  1024        block3_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 256, 41, 41)  32768       add_48[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, 256, 41, 41)  0           block3_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 256, 41, 41)  1024        conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_49 (Add)                    (None, 256, 41, 41)  0           block3_pool[0][0]                \n",
            "                                                                 batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_act (Activation (None, 256, 41, 41)  0           add_49[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1 (SeparableConv2 (None, 728, 41, 41)  188672      block4_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv1_bn (BatchNormal (None, 728, 41, 41)  2912        block4_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_act (Activation (None, 728, 41, 41)  0           block4_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2 (SeparableConv2 (None, 728, 41, 41)  536536      block4_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block4_sepconv2_bn (BatchNormal (None, 728, 41, 41)  2912        block4_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 728, 21, 21)  186368      add_49[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, 728, 21, 21)  0           block4_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 728, 21, 21)  2912        conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_50 (Add)                    (None, 728, 21, 21)  0           block4_pool[0][0]                \n",
            "                                                                 batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_act (Activation (None, 728, 21, 21)  0           add_50[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1 (SeparableConv2 (None, 728, 21, 21)  536536      block5_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv1_bn (BatchNormal (None, 728, 21, 21)  2912        block5_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_act (Activation (None, 728, 21, 21)  0           block5_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2 (SeparableConv2 (None, 728, 21, 21)  536536      block5_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv2_bn (BatchNormal (None, 728, 21, 21)  2912        block5_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_act (Activation (None, 728, 21, 21)  0           block5_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3 (SeparableConv2 (None, 728, 21, 21)  536536      block5_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block5_sepconv3_bn (BatchNormal (None, 728, 21, 21)  2912        block5_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_51 (Add)                    (None, 728, 21, 21)  0           block5_sepconv3_bn[0][0]         \n",
            "                                                                 add_50[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_act (Activation (None, 728, 21, 21)  0           add_51[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1 (SeparableConv2 (None, 728, 21, 21)  536536      block6_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv1_bn (BatchNormal (None, 728, 21, 21)  2912        block6_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_act (Activation (None, 728, 21, 21)  0           block6_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2 (SeparableConv2 (None, 728, 21, 21)  536536      block6_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv2_bn (BatchNormal (None, 728, 21, 21)  2912        block6_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_act (Activation (None, 728, 21, 21)  0           block6_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3 (SeparableConv2 (None, 728, 21, 21)  536536      block6_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block6_sepconv3_bn (BatchNormal (None, 728, 21, 21)  2912        block6_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_52 (Add)                    (None, 728, 21, 21)  0           block6_sepconv3_bn[0][0]         \n",
            "                                                                 add_51[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_act (Activation (None, 728, 21, 21)  0           add_52[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1 (SeparableConv2 (None, 728, 21, 21)  536536      block7_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv1_bn (BatchNormal (None, 728, 21, 21)  2912        block7_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_act (Activation (None, 728, 21, 21)  0           block7_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2 (SeparableConv2 (None, 728, 21, 21)  536536      block7_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv2_bn (BatchNormal (None, 728, 21, 21)  2912        block7_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_act (Activation (None, 728, 21, 21)  0           block7_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3 (SeparableConv2 (None, 728, 21, 21)  536536      block7_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block7_sepconv3_bn (BatchNormal (None, 728, 21, 21)  2912        block7_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_53 (Add)                    (None, 728, 21, 21)  0           block7_sepconv3_bn[0][0]         \n",
            "                                                                 add_52[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_act (Activation (None, 728, 21, 21)  0           add_53[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1 (SeparableConv2 (None, 728, 21, 21)  536536      block8_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv1_bn (BatchNormal (None, 728, 21, 21)  2912        block8_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_act (Activation (None, 728, 21, 21)  0           block8_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2 (SeparableConv2 (None, 728, 21, 21)  536536      block8_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv2_bn (BatchNormal (None, 728, 21, 21)  2912        block8_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_act (Activation (None, 728, 21, 21)  0           block8_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3 (SeparableConv2 (None, 728, 21, 21)  536536      block8_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block8_sepconv3_bn (BatchNormal (None, 728, 21, 21)  2912        block8_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_54 (Add)                    (None, 728, 21, 21)  0           block8_sepconv3_bn[0][0]         \n",
            "                                                                 add_53[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_act (Activation (None, 728, 21, 21)  0           add_54[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1 (SeparableConv2 (None, 728, 21, 21)  536536      block9_sepconv1_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv1_bn (BatchNormal (None, 728, 21, 21)  2912        block9_sepconv1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_act (Activation (None, 728, 21, 21)  0           block9_sepconv1_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2 (SeparableConv2 (None, 728, 21, 21)  536536      block9_sepconv2_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv2_bn (BatchNormal (None, 728, 21, 21)  2912        block9_sepconv2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_act (Activation (None, 728, 21, 21)  0           block9_sepconv2_bn[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3 (SeparableConv2 (None, 728, 21, 21)  536536      block9_sepconv3_act[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block9_sepconv3_bn (BatchNormal (None, 728, 21, 21)  2912        block9_sepconv3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_55 (Add)                    (None, 728, 21, 21)  0           block9_sepconv3_bn[0][0]         \n",
            "                                                                 add_54[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_act (Activatio (None, 728, 21, 21)  0           add_55[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1 (SeparableConv (None, 728, 21, 21)  536536      block10_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv1_bn (BatchNorma (None, 728, 21, 21)  2912        block10_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_act (Activatio (None, 728, 21, 21)  0           block10_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2 (SeparableConv (None, 728, 21, 21)  536536      block10_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv2_bn (BatchNorma (None, 728, 21, 21)  2912        block10_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_act (Activatio (None, 728, 21, 21)  0           block10_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3 (SeparableConv (None, 728, 21, 21)  536536      block10_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block10_sepconv3_bn (BatchNorma (None, 728, 21, 21)  2912        block10_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_56 (Add)                    (None, 728, 21, 21)  0           block10_sepconv3_bn[0][0]        \n",
            "                                                                 add_55[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_act (Activatio (None, 728, 21, 21)  0           add_56[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1 (SeparableConv (None, 728, 21, 21)  536536      block11_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv1_bn (BatchNorma (None, 728, 21, 21)  2912        block11_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_act (Activatio (None, 728, 21, 21)  0           block11_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2 (SeparableConv (None, 728, 21, 21)  536536      block11_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv2_bn (BatchNorma (None, 728, 21, 21)  2912        block11_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_act (Activatio (None, 728, 21, 21)  0           block11_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3 (SeparableConv (None, 728, 21, 21)  536536      block11_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block11_sepconv3_bn (BatchNorma (None, 728, 21, 21)  2912        block11_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_57 (Add)                    (None, 728, 21, 21)  0           block11_sepconv3_bn[0][0]        \n",
            "                                                                 add_56[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_act (Activatio (None, 728, 21, 21)  0           add_57[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1 (SeparableConv (None, 728, 21, 21)  536536      block12_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv1_bn (BatchNorma (None, 728, 21, 21)  2912        block12_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_act (Activatio (None, 728, 21, 21)  0           block12_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2 (SeparableConv (None, 728, 21, 21)  536536      block12_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv2_bn (BatchNorma (None, 728, 21, 21)  2912        block12_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_act (Activatio (None, 728, 21, 21)  0           block12_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3 (SeparableConv (None, 728, 21, 21)  536536      block12_sepconv3_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block12_sepconv3_bn (BatchNorma (None, 728, 21, 21)  2912        block12_sepconv3[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_58 (Add)                    (None, 728, 21, 21)  0           block12_sepconv3_bn[0][0]        \n",
            "                                                                 add_57[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_act (Activatio (None, 728, 21, 21)  0           add_58[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1 (SeparableConv (None, 728, 21, 21)  536536      block13_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv1_bn (BatchNorma (None, 728, 21, 21)  2912        block13_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_act (Activatio (None, 728, 21, 21)  0           block13_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2 (SeparableConv (None, 1024, 21, 21) 752024      block13_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block13_sepconv2_bn (BatchNorma (None, 1024, 21, 21) 4096        block13_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 1024, 11, 11) 745472      add_58[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block13_pool (MaxPooling2D)     (None, 1024, 11, 11) 0           block13_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 1024, 11, 11) 4096        conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_59 (Add)                    (None, 1024, 11, 11) 0           block13_pool[0][0]               \n",
            "                                                                 batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1 (SeparableConv (None, 1536, 11, 11) 1582080     add_59[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_bn (BatchNorma (None, 1536, 11, 11) 6144        block14_sepconv1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv1_act (Activatio (None, 1536, 11, 11) 0           block14_sepconv1_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2 (SeparableConv (None, 2048, 11, 11) 3159552     block14_sepconv1_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_bn (BatchNorma (None, 2048, 11, 11) 8192        block14_sepconv2[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "block14_sepconv2_act (Activatio (None, 2048, 11, 11) 0           block14_sepconv2_bn[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_4 (Glo (None, 2048)         0           block14_sepconv2_act[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "prediction (Dense)              (None, 5)            10245       global_average_pooling2d_4[0][0] \n",
            "==================================================================================================\n",
            "Total params: 20,871,725\n",
            "Trainable params: 20,817,197\n",
            "Non-trainable params: 54,528\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dMfenMQcxAAb"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M-ID7vP5mIKs",
        "outputId": "6abd5509-e666-4368-9b4a-0e2ccb0f08b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "print ('BATCH_SIZE: ', BATCH_SIZE)\n",
        "start_time = time.time()\n",
        "history = model.fit(training_dataset, validation_data=validation_dataset,\n",
        "                    steps_per_epoch=TRAIN_STEPS, epochs=EPOCHS, callbacks=[lr_callback])\n",
        "\n",
        "final_accuracy = history.history[\"val_accuracy\"][-5:]\n",
        "print(\"FINAL ACCURACY MEAN-5: \", np.mean(final_accuracy))\n",
        "print(\"TRAINING TIME: \", time.time() - start_time, \" sec\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BATCH_SIZE:  48\n",
            "Train for 62 steps\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
            "Epoch 1/12\n",
            "62/62 [==============================] - 97s 2s/step - loss: 1.3634 - accuracy: 0.5326 - val_loss: 1.1814 - val_accuracy: 0.7132\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 4.8e-05.\n",
            "Epoch 2/12\n",
            "62/62 [==============================] - 80s 1s/step - loss: 0.5553 - accuracy: 0.8713 - val_loss: 0.3196 - val_accuracy: 0.9029\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 8.6e-05.\n",
            "Epoch 3/12\n",
            "62/62 [==============================] - 80s 1s/step - loss: 0.1795 - accuracy: 0.9523 - val_loss: 0.1620 - val_accuracy: 0.9559\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.000124.\n",
            "Epoch 4/12\n",
            "39/62 [=================>............] - ETA: 28s - loss: 0.0944 - accuracy: 0.9754"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VngeUBIdyJ1T",
        "colab": {}
      },
      "source": [
        "print(history.history.keys())\n",
        "display_training_curves(history.history['accuracy'][1:], history.history['val_accuracy'][1:], 'accuracy', 211)\n",
        "display_training_curves(history.history['loss'][1:], history.history['val_loss'][1:], 'loss', 212)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MKFMWzh0Yxsq"
      },
      "source": [
        "## Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZcpoO4jOB-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a couple of images to test predictions too\n",
        "some_flowers, some_labels = dataset_to_numpy_util(validation_dataset, 160)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2X07EKsqK_RW",
        "colab": {}
      },
      "source": [
        "# randomize the input so that you can execute multiple times to change results\n",
        "permutation = np.random.permutation(8*20)\n",
        "some_flowers, some_labels = (some_flowers[permutation], some_labels[permutation])\n",
        "\n",
        "predictions = model.predict(some_flowers, batch_size=16)\n",
        "evaluations = model.evaluate(some_flowers, some_labels, batch_size=16)\n",
        "  \n",
        "print(np.array(CLASSES)[np.argmax(predictions, axis=-1)].tolist())\n",
        "print('[val_loss, val_acc]', evaluations)\n",
        "\n",
        "display_9_images_with_predictions(some_flowers, predictions, some_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIC5KKcJOB_A",
        "colab_type": "text"
      },
      "source": [
        "## Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivTV2qPoOB_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_model = create_model()\n",
        "save_model.set_weights(model.get_weights())\n",
        "save_model.save('model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOVgGwS-OB_K",
        "colab_type": "text"
      },
      "source": [
        "## Reload the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4AAIjcoOB_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reload_model = tf.keras.models.load_model('model.h5')\n",
        "\n",
        "predictions = reload_model.predict(some_flowers, batch_size=16)\n",
        "evaluations = reload_model.evaluate(some_flowers, some_labels, batch_size=16)\n",
        "print(np.array(CLASSES)[np.argmax(predictions, axis=-1)].tolist())\n",
        "print('[val_loss, val_acc]', evaluations)\n",
        "display_9_images_with_predictions(some_flowers, predictions, some_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hleIN5-pcr0N"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "author: Martin Gorner<br>\n",
        "twitter: @martin_gorner\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Copyright 2019 Google LLC\n",
        "\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "you may not use this file except in compliance with the License.\n",
        "You may obtain a copy of the License at\n",
        "\n",
        "    http://www.apache.org/licenses/LICENSE-2.0\n",
        "\n",
        "Unless required by applicable law or agreed to in writing, software\n",
        "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "See the License for the specific language governing permissions and\n",
        "limitations under the License.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is not an official Google product but sample code provided for an educational purpose\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KErXKoZFQUI",
        "colab_type": "code",
        "outputId": "28ab30a6-968f-4005-99c3-55da9d31189c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset = validation_dataset.unbatch().batch(10)\n",
        "i=0\n",
        "for images, labels in dataset:\n",
        "  numpy_images = images.numpy()\n",
        "  numpy_labels = labels.numpy()\n",
        "  i += numpy_labels.shape[0]\n",
        "  \n",
        "  \n",
        "  #break;  \n",
        "print (i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "680\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUJQFoEZFxRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}