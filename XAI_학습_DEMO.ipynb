{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XAI_학습_DEMO",
      "provenance": [],
      "collapsed_sections": [
        "clSFHJkFNylD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iskra3138/ImageSr/blob/master/XAI_%ED%95%99%EC%8A%B5_DEMO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FpvUOuC3j27n",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title DEMO 사전 준비 [Run Me!!!!!]\n",
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "\n",
        "try:\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "except ValueError:\n",
        "  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n",
        "\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "BUCKET = \"gs://iskra3138_share\"\n",
        "\n",
        "\n",
        "import cv2\n",
        "\n",
        "class GradCAM:\n",
        "  def __init__(self, model, activation_layer):\n",
        "    self.model = model\n",
        "    self.activation_layer = activation_layer\n",
        "    self.tensor_function = self._get_gradcam_tensor_function()\n",
        "\n",
        "  # get partial tensor graph of CNN model\n",
        "  def _get_gradcam_tensor_function(self):\n",
        "    model_input = self.model.input\n",
        "    y_c = self.model.output\n",
        "    A_k = self.model.get_layer(self.activation_layer).output\n",
        "\n",
        "    tensor_function = tf.keras.models.Model([model_input], [A_k, y_c])\n",
        "    return tensor_function\n",
        "\n",
        "  # generate Grad-CAM\n",
        "  def generate(self, input_tensor):\n",
        "    preds = self.model.predict(input_tensor)[0]\n",
        "    class_idx = np.argmax(preds)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "      conv_outputs, predictions = self.tensor_function(input_tensor) \n",
        "      loss = predictions[:, class_idx]\n",
        "\n",
        "    output = conv_outputs[0]\n",
        "    \n",
        "    grads = tape.gradient(loss, conv_outputs)[0]    \n",
        "    weights = np.mean(grads, axis=(0, 1))\n",
        "    \n",
        "    grad_cam = np.dot(output, weights)\n",
        "\n",
        "    grad_cam = np.maximum(grad_cam, 0)\n",
        "    grad_cam = cv2.resize(grad_cam, (224, 224))\n",
        "    return grad_cam, preds, class_idx\n",
        "  \n",
        "  \n",
        "import os\n",
        "\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "\n",
        "IMG_WIDTH = 224\n",
        "IMG_HEIGHT = 224\n",
        "IMAGE_SIZE =  [IMG_HEIGHT, IMG_WIDTH]\n",
        "\n",
        "batch_size = 8 * tpu_strategy.num_replicas_in_sync\n",
        "\n",
        "## 본 실험에서는 16개의 tfrecord파일을 train/validation용으로 나눠서 사용합니다.\n",
        "## train전용, validation전용 tfrecord 파일들이 있으면 특정해서 list 로 넘기시면 됩니다.\n",
        "gcs_pattern = os.path.join(BUCKET, '*.tfrec')\n",
        "validation_split = 0.19\n",
        "filenames = tf.io.gfile.glob(gcs_pattern)\n",
        "split = len(filenames) - int(len(filenames) * validation_split)\n",
        "train_fns = filenames[:split]\n",
        "validation_fns = filenames[split:]\n",
        "\n",
        "def parse_tfrecord(example):\n",
        "    features = {\n",
        "        \"image\": tf.io.FixedLenFeature([], tf.string),  # tf.string = bytestring (not text string)\n",
        "        \"file_name\": tf.io.FixedLenFeature([], tf.string),  # one bytestring\n",
        "        \"label_name\": tf.io.FixedLenFeature([], tf.string),  # one bytestring\n",
        "        \"label\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means scalar, one integer\n",
        "    }\n",
        "    # decode the TFRecord\n",
        "    example = tf.io.parse_single_example(example, features)\n",
        "    \n",
        "    # FixedLenFeature fields are now ready to use: exmple['size']\n",
        "    # VarLenFeature fields require additional sparse_to_dense decoding\n",
        "    \n",
        "    label = example['label']\n",
        "    label = tf.one_hot(indices=label,\n",
        "                      depth=5\n",
        "                      )   \n",
        "    image = tf.image.decode_jpeg(example['image'], channels=3)\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32) ## make [0,255] to [0,1) resize 앞에 위치할 때만 [0,1), 즉 input이 float32가 아니어야 작동\n",
        "    image = tf.image.resize(image, IMAGE_SIZE) ## method가 tf.image.ResizeMethod.NEAREST_NEIGHBOR 가 아니면 출력은 무조건 float32\n",
        "    \n",
        "    #file_name  = example['file_name']\n",
        "    #label_name  = example['label_name']\n",
        "    \n",
        "    return image, label\n",
        "\n",
        "def load_dataset(filenames):\n",
        "  # Read from TFRecords. For optimal performance, we interleave reads from multiple files.\n",
        "  records = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO)\n",
        "  return records.map(parse_tfrecord, num_parallel_calls=AUTO)\n",
        "\n",
        "def get_training_dataset():\n",
        "  dataset = load_dataset(train_fns)\n",
        "\n",
        "  # Create some additional training images by randomly flipping and\n",
        "  # increasing/decreasing the saturation of images in the training set. \n",
        "  def data_augment(image, label):\n",
        "    modified = tf.image.random_flip_left_right(image)\n",
        "    modified = tf.image.random_flip_up_down(modified)\n",
        "    return modified, label\n",
        "  augmented = dataset.map(data_augment, num_parallel_calls=AUTO)\n",
        "\n",
        "  # Prefetch the next batch while training (autotune prefetch buffer size).\n",
        "  return augmented.repeat().shuffle(2048).batch(batch_size).prefetch(AUTO) \n",
        "\n",
        "training_dataset = get_training_dataset()\n",
        "validation_dataset = load_dataset(validation_fns).batch(batch_size).prefetch(AUTO)\n",
        "\n",
        "CLASSES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n",
        "\n",
        "def display_one(image, title, subplot, color):\n",
        "  plt.subplot(subplot)\n",
        "  plt.axis('off')\n",
        "  plt.imshow(image)\n",
        "  plt.title(title, fontsize=16, color=color)\n",
        "  \n",
        "# If model is provided, use it to generate predictions.\n",
        "def display_nine(images, titles, title_colors=None):\n",
        "  subplot = 331\n",
        "  plt.figure(figsize=(13,13))\n",
        "  for i in range(9):\n",
        "    color = 'black' if title_colors is None else title_colors[i]\n",
        "    display_one(images[i], titles[i], 331+i, color)\n",
        "  plt.tight_layout()\n",
        "  plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
        "  plt.show()\n",
        "\n",
        "def get_dataset_iterator(dataset, n_examples):\n",
        "  return dataset.unbatch().batch(n_examples).as_numpy_iterator()\n",
        "\n",
        "training_viz_iterator = get_dataset_iterator(training_dataset, 9)\n",
        "\n",
        "def create_model():\n",
        "  #pretrained_model = tf.keras.applications.ResNet101(weights='imagenet', input_shape=[*IMAGE_SIZE, 3], include_top=False)\n",
        "  pretrained_model = tf.keras.applications.Xception(input_shape=[*IMAGE_SIZE, 3], include_top=False)\n",
        "  pretrained_model.trainable = True\n",
        "  x = pretrained_model.output\n",
        "  x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "  predictions = tf.keras.layers.Dense(5, activation='softmax', name='prediction')(x)\n",
        "  model = tf.keras.Model(inputs=pretrained_model.input, outputs=predictions)\n",
        "  \n",
        "  #optimizer='adam',\n",
        "  #optimizer=tf.compat.v1.train.AdamOptimizer(learning_rate=1e-4),\n",
        "    \n",
        "  model.compile(\n",
        "    optimizer='adam',  \n",
        "    loss = 'categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        "  )\n",
        "  return model\n",
        "\n",
        "with tpu_strategy.scope(): # creating the model in the TPUStrategy scope means we will train the model on the TPU\n",
        "  model = create_model()\n",
        "weights = model.get_weights()\n",
        "\n",
        "def count_data_items(filenames):\n",
        "  # The number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n",
        "  n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
        "  return np.sum(n)\n",
        "\n",
        "n_train = count_data_items(train_fns)\n",
        "n_valid = count_data_items(validation_fns)\n",
        "\n",
        "train_steps = n_train // batch_size\n",
        "valid_steps = n_valid // batch_size\n",
        "#train_steps = count_data_items(train_fns) // batch_size\n",
        "#valid_steps = count_data_items(validation_fns) // batch_size\n",
        "\n",
        "print(\"TRAINING IMAGES: \", n_train, \", STEPS PER EPOCH: \", train_steps)\n",
        "print(\"VALIDATION IMAGES: \", n_valid, \", STEPS PER EPOCH: \", valid_steps)\n",
        "\n",
        "activation_layer = 'block14_sepconv2_act' \n",
        "\n",
        "EPOCHS = 12\n",
        "\n",
        "start_lr = 0.00001\n",
        "min_lr = 0.00001\n",
        "max_lr = 0.00005 * tpu_strategy.num_replicas_in_sync\n",
        "rampup_epochs = 5\n",
        "sustain_epochs = 0\n",
        "exp_decay = .8\n",
        "\n",
        "def lrfn(epoch):\n",
        "  if epoch < rampup_epochs:\n",
        "    return (max_lr - start_lr)/rampup_epochs * epoch + start_lr\n",
        "  elif epoch < rampup_epochs + sustain_epochs:\n",
        "    return max_lr\n",
        "  else:\n",
        "    return (max_lr - min_lr) * exp_decay**(epoch-rampup_epochs-sustain_epochs) + min_lr\n",
        "    \n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=True)\n",
        "\n",
        "rang = np.arange(EPOCHS)\n",
        "y = [lrfn(x) for x in rang]\n",
        "plt.plot(rang, y)\n",
        "print('Learning rate per epoch:')\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49mlX3II7Z2i",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title Class별 Sampling [Run Me!!!!]\n",
        "\n",
        "from google.colab import widgets\n",
        "\n",
        "## 5개의 클래스마다 1장씩 샘플링 합니다.\n",
        "sample_images=[]\n",
        "sample_labels=[]\n",
        "for i in range(5):\n",
        "  def sampling(image, label):\n",
        "    return tf.math.equal(tf.argmax(label, axis=0), i)\n",
        "    #return tf.argmax(label, axis=0) == i\n",
        "\n",
        "  sample_iterator = load_dataset(validation_fns).shuffle(100).filter(sampling).batch(1).as_numpy_iterator()\n",
        "  image, label = next(sample_iterator)\n",
        "  for j in range(1):\n",
        "    sample_images.append(image[j])\n",
        "    sample_labels.append(label[j])\n",
        "        \n",
        "gradcam_gen = GradCAM(model, activation_layer)\n",
        "\n",
        "grid = widgets.Grid(2, 5, header_row=True, header_column=True)\n",
        "grads, preds, class_idxs = [], [], []\n",
        "for i in range(5):\n",
        "  img_tensor = np.expand_dims(sample_images[i], axis=0)\n",
        "  grad, pred, class_idx = gradcam_gen.generate(img_tensor)\n",
        "  grads.append(grad)\n",
        "  preds.append(pred)\n",
        "  class_idxs.append(class_idx)\n",
        "\n",
        "for n in range (5) :\n",
        "  with grid.output_to(0, n):\n",
        "    plt.imshow(sample_images[n])\n",
        "    plt.title(CLASSES[np.argmax(sample_labels[n])].title())\n",
        "    plt.axis('off')\n",
        "  with grid.output_to(1, n):\n",
        "    for pred in preds[n]:\n",
        "      print ('{:.2f}'.format(pred), end=',')\n",
        "    print ('\\n')\n",
        "      \n",
        "    plt.imshow(sample_images[n])\n",
        "    plt.imshow(grads[n], cmap='jet', alpha=0.5)\n",
        "    plt.title(CLASSES[class_idxs[n]].title())\n",
        "    plt.axis('off')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_53Jk02oqSD4",
        "colab_type": "text"
      },
      "source": [
        "# 학습 DEMO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VjneDu60AV6",
        "colab_type": "code",
        "colab": {},
        "cellView": "form"
      },
      "source": [
        "#@title 학습 DEMO [Run ME!!!]\n",
        "from google.colab import widgets\n",
        "\n",
        "model.set_weights(weights)\n",
        "for i in range(12):\n",
        "  history = model.fit(training_dataset, validation_data=validation_dataset,\n",
        "                    steps_per_epoch=train_steps, epochs=i+1, initial_epoch=i, callbacks=[lr_callback])\n",
        "  \n",
        "  gradcam_gen = GradCAM(model, activation_layer)\n",
        "  \n",
        "  grid = widgets.Grid(2, 5, header_row=True, header_column=True)\n",
        "  grads, preds, class_idxs = [], [], []\n",
        "  for i in range(5):\n",
        "    img_tensor = np.expand_dims(sample_images[i], axis=0)\n",
        "    grad, pred, class_idx = gradcam_gen.generate(img_tensor)\n",
        "    grads.append(grad)\n",
        "    preds.append(pred)\n",
        "    class_idxs.append(class_idx)\n",
        "\n",
        "  for n in range (5) :\n",
        "    with grid.output_to(0, n):\n",
        "      plt.imshow(sample_images[n])\n",
        "      plt.title(CLASSES[np.argmax(sample_labels[n])].title())\n",
        "      plt.axis('off')\n",
        "    with grid.output_to(1, n):\n",
        "      for pred in preds[n]:\n",
        "        print ('{:.2f}'.format(pred), end=',')\n",
        "      print ('\\n')\n",
        "\n",
        "      plt.imshow(sample_images[n])\n",
        "      plt.imshow(grads[n], cmap='jet', alpha=0.5)\n",
        "      plt.title(CLASSES[class_idxs[n]].title())\n",
        "      plt.axis('off')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwDfHGxOTJOe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}